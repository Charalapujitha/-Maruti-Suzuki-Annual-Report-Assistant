# -*- coding: utf-8 -*-
"""Maruti_nlp_project.2ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yRMuvHaXYnx3uyzn3debaFGNNMQIu1dD
"""

import nltk

nltk.download('punkt')

nltk.download('stopwords')

nltk.download('wordnet')

!pip install pymupdf

import fitz  # PyMuPDF
import pandas as pd

# Path to your PDF file
pdf_path = "/MARUTI-Maruti_Suzuki_India_Ltd-AnnualReport-FY2024.pdf"
doc = fitz.open(pdf_path)

# Create a list to hold page data
page_data = []

# Loop through each page of the PDF
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    text = page.get_text()
    page_data.append({'page_number': page_num + 1, 'text': text})

# Create the DataFrame
df = pd.DataFrame(page_data)

print("PDF imported successfully!")
df.head()

import re
import string
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove digits
    text = re.sub(r'\d+', '', text)
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove special characters and extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    # Remove stopwords
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Apply the preprocessing function to the 'text' column
df['processed_text'] = df['text'].apply(preprocess_text)

print("Text preprocessing complete.")
df[['text', 'processed_text']].head()

from nltk.tokenize import sent_tokenize
from textblob import TextBlob

sentiment_data = []

# Iterate over each page's original text (before stopword removal)
for index, row in df.iterrows():
    sentences = sent_tokenize(row['text'])
    for sentence in sentences:
        blob = TextBlob(sentence)
        sentiment_data.append({
            'page_number': row['page_number'],
            'sentence': sentence,
            'polarity': blob.sentiment.polarity,      # Polarity score (-1 to 1)
            'subjectivity': blob.sentiment.subjectivity # Subjectivity score (0 to 1)
        })

# Create a new DataFrame for sentiment analysis results
sentiment_df = pd.DataFrame(sentiment_data)

print("Sentiment analysis complete.")
sentiment_df.head()

import nltk
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Combine all processed text into one large block
full_text = ' '.join(df['processed_text'])

# Tokenize the full text into words
tokens = word_tokenize(full_text)

# Lemmatize the tokens
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]

print(f"Total tokens after lemmatization: {len(lemmatized_tokens)}")
print("First 20 tokens:", lemmatized_tokens[:20])

from nltk.probability import FreqDist
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Calculate word frequencies
fdist = FreqDist(lemmatized_tokens)
print("Top 20 most frequent words:")
print(fdist.most_common(20))

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(fdist)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer

# Use the processed text from each page as a document
documents = df['processed_text'].tolist()

# Initialize the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1000) # Limiting to top 1000 features

# Fit and transform the documents
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

print("TF-IDF matrix created with shape:", tfidf_matrix.shape)

import gensim
from gensim import corpora
from nltk.tokenize import word_tokenize

# Re-tokenize the processed_text column for LDA
texts = [word_tokenize(text) for text in df['processed_text']]

# Create a dictionary and a corpus required by Gensim
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Build the LDA model
# The incorrect comment has been removed from the num_topics line
lda_model = gensim.models.ldamodel.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=10,  # As required by the project
    random_state=100,
    update_every=1,
    chunksize=100,
    passes=10,
    alpha='auto',
    per_word_topics=True
)

# Print the 10 discovered topics
print("Discovered Topics: ðŸ“œ\n")
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic: {idx} \nWords: {topic}\n")

!pip install gensim

"""from IPython.display import display, HTML

summary_html = '''
<div style="
    border: 3px solid #4A90E2;
    border-radius: 15px;
    padding: 25px;
    background: linear-gradient(135deg, #f0f8ff, #ffffff);
    font-family: 'Segoe UI', Arial, sans-serif;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
">
  <h2 style="
      color: #003366;
      text-align: center;
      margin-bottom: 15px;
      border-bottom: 2px solid #4A90E2;
      padding-bottom: 10px;
  ">
      ðŸš— NLP Analysis Report â€“ Maruti Suzuki Pvt Ltd (FY 2024â€“25)
  </h2>
  <p style="font-size: 15px; text-align: justify;">
      This project applied advanced <strong>Natural Language Processing (NLP)</strong> techniques
      to analyze the <strong>Maruti Suzuki Pvt Ltd Annual Report 2024â€“25</strong>.
      The analysis converted the full document into structured data,
      revealing the companyâ€™s strategic, financial, and operational insights.
  </p>
  <ul style="font-size: 14px; line-height: 1.6;">
      <li><strong>ðŸ“ˆ Sentiment Analysis:</strong> The report showed an overall <strong>positive tone</strong>,
      reflecting steady growth, customer satisfaction, and innovation efforts.</li>
      <li><strong>ðŸ’¡ Topic Modeling (LDA):</strong> The system discovered <strong>10 main themes</strong> â€”
      including manufacturing performance, sustainability, product development, and future business outlook.</li>
  </ul>
  <div style="
      background-color: #e8f4ff;
      border-left: 6px solid #4A90E2;
      padding: 15px;
      margin-top: 20px;
  ">
      <p style="margin: 0; font-style: italic; color: #003366;">
          <strong>Conclusion:</strong> NLP enables automation of insights from large corporate reports,
          helping summarize key business narratives with accuracy and clarity.
      </p>
  </div>
</div>
'''

display(HTML(summary_html))

"""

# RUN THIS CELL AGAIN IN YOUR NOTEBOOK

import json
import pandas as pd
import numpy as np

def convert_types(obj):
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_types(i) for i in obj]
    return obj

# Prepare data
pages_data = df[['page_number', 'text']].to_dict(orient='records')
vocabulary = tfidf_vectorizer.vocabulary_
tfidf_matrix_dense = tfidf_matrix.toarray().tolist()

chatbot_data_raw = {
    "pages": pages_data,
    "vocabulary": vocabulary,
    "tfidf_matrix": tfidf_matrix_dense
}

chatbot_data = convert_types(chatbot_data_raw)

# Save as JSON instead of JS
file_path = 'chatbot_data.json'
with open(file_path, 'w', encoding='utf-8') as f:
    json.dump(chatbot_data, f, ensure_ascii=False, indent=4)

print(f"âœ… SUCCESS: The file 'chatbot_data.json' has been created with all the data!")

